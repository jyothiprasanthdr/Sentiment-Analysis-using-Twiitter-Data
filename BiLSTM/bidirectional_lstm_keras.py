# -*- coding: utf-8 -*-
"""bidirectional-lstm-keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16vrX5TQdh6YNBJI9V9DsY8rwarZvbAN6
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)


#from google.colab import drive
#drive.mount('/content/drive')

import sys, os, re, csv, codecs, numpy as np, pandas as pd

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers

EMBEDDING_FILE=open('glove.6B.100d.txt')
TRAIN_DATA_FILE=open('User history.csv')
#TEST_DATA_FILE=f'Test2.csv'

#set parameters
embed_size = 100 # how big is each word vector
max_features = 11700 # how many unique words to use (i.e num rows in embedding vector)
maxlen = 1000 # max number of words in a comment to use

#read datasets
train = pd.read_csv(TRAIN_DATA_FILE, encoding ='unicode-escape')
#test = pd.read_csv(TEST_DATA_FILE, encoding='unicode-escape')

"""pre-processing"""

list_sentences_train = train["Userhistory"].fillna("_na_").values
list_classes = ["label"]
y = train[list_classes].values
#list_sentences_test = test["Tweet"].fillna("_na_").values

tokenizer = Tokenizer(num_words=max_features)         #Tokenization to find maximum features
tokenizer.fit_on_texts(list(list_sentences_train))
list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)
#list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)
X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)
#X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)

"""Read the glove word vectors (space delimited strings) into a dictionary from word->vector."""

def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')
embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))

"""Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init."""

all_embs = np.stack(embeddings_index.values())
emb_mean,emb_std = all_embs.mean(), all_embs.std()
emb_mean,emb_std

word_index = tokenizer.word_index
nb_words = min(max_features, len(word_index))
embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))
for word, i in word_index.items():
    if i >= max_features: continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None: embedding_matrix[i] = embedding_vector

"""bidirectional LSTM"""

inp = Input(shape=(maxlen,))
x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)
x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(x)
x = GlobalMaxPool1D()(x)
x = Dense(100, activation="relu")(x)
x = Dropout(0.25)(x)
x = Dense(1, activation="sigmoid")(x)
model = Model(inputs=inp, outputs=x)
#adam =Adam(0.001)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_t, y, batch_size=128, epochs=1 ,validation_split=0.2)

pred = model.predict(X_t, batch_size=100, verbose=1)
print(pred)

print(min(pred))
print(max(pred))

np.arrays=pred
pred[pred >0.40]=int(1)
pred[pred < 0.40]=int(0)

pred.astype(int)

from sklearn.metrics import classification_report 
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import accuracy_score
results = confusion_matrix(y,pred.astype(int)) 
  
print ('Confusion Matrix :')
print(results) 
print ('Accuracy Score :',accuracy_score(y, pred.astype(int) ))
print ('Report : ')
print (classification_report(y,pred.astype(int) ))

